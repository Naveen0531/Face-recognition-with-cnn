{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D-hyr1gpM6hp"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # For numerical computations\n",
        "import os  # Provides operating system functionalities\n",
        "from sklearn.datasets import fetch_lfw_people  # For loading the LFW dataset\n",
        "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
        "from sklearn.metrics import classification_report, accuracy_score  # For evaluating model performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and Preprocess Data\n",
        "def load_and_preprocess_data():\n",
        "    # Load LFW dataset, filtering people with at least 70 images and resizing them\n",
        "    data = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
        "    X, y = data.images, data.target  # Extract image data and corresponding labels\n",
        "    target_names = data.target_names  # Get the names of the classes\n",
        "\n",
        "    # Normalize pixel values to the range [0, 1]\n",
        "    X = X / 255.0\n",
        "\n",
        "    return X, y, target_names"
      ],
      "metadata": {
        "id": "EILAxy8aNB_j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. ResNet Implementation\n",
        "class BasicResNetBlock:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        # Initialize weights and biases for the block\n",
        "        self.W = np.random.randn(input_dim, output_dim) * 0.1  # Small random weights\n",
        "        self.b = np.zeros(output_dim)  # Bias initialized to zeros\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Linear transformation: Z = X * W + b\n",
        "        Z = np.dot(X, self.W) + self.b # this performs a linear transformation on the input X using the weights self.W and biasesÂ self.b.\n",
        "        # ReLU activation: A = max(0, Z)\n",
        "        A = np.maximum(0, Z) # this applies the ReLU activation function to the linear output 'Z'\n",
        "        return A\n",
        "\n",
        "class ResNet:\n",
        "    def __init__(self, input_dim, output_dim, num_blocks):\n",
        "        self.blocks = []  # List to hold all ResNet blocks\n",
        "        current_dim = np.prod(input_dim)  # Calculates the flattened input size (e.g., height * width * channels)for compatability with fully connected layers\n",
        "\n",
        "        # Create intermediate blocks\n",
        "        for _ in range(num_blocks - 1):\n",
        "            self.blocks.append(BasicResNetBlock(current_dim, current_dim))\n",
        "\n",
        "        # Final block outputs the required dimension\n",
        "        self.blocks.append(BasicResNetBlock(current_dim, output_dim))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Flatten input for compatibility with fully connected layers\n",
        "        X_flat = X.reshape(X.shape[0], -1)\n",
        "        output = X_flat\n",
        "        # Pass input through each block sequentially\n",
        "        for block in self.blocks:\n",
        "            output = block.forward(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "yB8uAibkNGAc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Decision Tree Implementation\n",
        "class DecisionTreeNode:\n",
        "    def __init__(self, depth=0, max_depth=10, label=None):\n",
        "        self.depth = depth  # Depth of the node in the tree\n",
        "        self.max_depth = max_depth  # Maximum depth allowed for the tree\n",
        "        self.left = None  # Left child node\n",
        "        self.right = None  # Right child node\n",
        "        self.feature_index = None  # Index of the feature used for splitting\n",
        "        self.threshold = None  # Threshold value for splitting\n",
        "        self.label = label  # Class label if this is a leaf node\n",
        "\n",
        "    def is_leaf(self):\n",
        "        # Check if this node is a leaf\n",
        "        return self.label is not None\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=10):\n",
        "        self.max_depth = max_depth  # Maximum depth for the tree\n",
        "        self.root = None  # Root node of the tree\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Build the decision tree recursively\n",
        "        self.root = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape  # Get the number of samples and features\n",
        "        if depth >= self.max_depth or len(set(y)) == 1:\n",
        "            # Stop growing and create a leaf node if maximum depth is reached or only one class remains\n",
        "            return DecisionTreeNode(depth=depth, label=max(set(y), key=list(y).count))\n",
        "\n",
        "        # Find the best feature and threshold to split the data\n",
        "        best_feature, best_threshold = self._best_split(X, y)\n",
        "        if best_feature is None:\n",
        "            # If no valid split is found, create a leaf node\n",
        "            return DecisionTreeNode(depth=depth, label=max(set(y), key=list(y).count))\n",
        "\n",
        "        # Split the data into left and right subsets\n",
        "        left_indices = X[:, best_feature] <= best_threshold\n",
        "        right_indices = X[:, best_feature] > best_threshold\n",
        "\n",
        "        # Create a new node and recursively grow its left and right children\n",
        "        node = DecisionTreeNode(depth=depth)\n",
        "        node.feature_index = best_feature\n",
        "        node.threshold = best_threshold\n",
        "        node.left = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        node.right = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "        return node\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        num_samples, num_features = X.shape  # Get dimensions of the dataset\n",
        "        if num_samples <= 1:\n",
        "            return None, None\n",
        "\n",
        "        best_gini = float('inf')  # Initialize best Gini impurity to a large value\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        # Iterate over all features\n",
        "        for feature_index in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_index])  # Unique values of the feature\n",
        "            for threshold in thresholds:\n",
        "                # Split the data\n",
        "                left_indices = X[:, feature_index] <= threshold\n",
        "                right_indices = X[:, feature_index] > threshold\n",
        "\n",
        "                # Skip invalid splits\n",
        "                if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Compute Gini impurity for the split\n",
        "                gini = self._gini_impurity(y[left_indices], y[right_indices])\n",
        "\n",
        "                # Update the best split if a lower Gini impurity is found\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_feature = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _gini_impurity(self, left_y, right_y):\n",
        "        # Helper function to calculate Gini impurity\n",
        "        def gini(y):\n",
        "            m = len(y)\n",
        "            if m == 0:\n",
        "                return 0\n",
        "            counts = np.bincount(y)  # Count occurrences of each class\n",
        "            probabilities = counts / m  # Convert counts to probabilities\n",
        "            return 1 - np.sum(probabilities**2)  # Gini formula\n",
        "\n",
        "        m_left = len(left_y)\n",
        "        m_right = len(right_y)\n",
        "        m_total = m_left + m_right\n",
        "\n",
        "        # Weighted Gini impurity of the split\n",
        "        gini_left = gini(left_y)\n",
        "        gini_right = gini(right_y)\n",
        "\n",
        "        return (m_left / m_total) * gini_left + (m_right / m_total) * gini_right\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Predict class labels for each sample in X\n",
        "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        # Traverse the tree to make a prediction for a single sample\n",
        "        if node.is_leaf():\n",
        "            return node.label\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._traverse_tree(x, node.left)\n",
        "        return self._traverse_tree(x, node.right)\n"
      ],
      "metadata": {
        "id": "MMPZ7wUJNJal"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train and Evaluate\n",
        "def main():\n",
        "    # Load and preprocess data\n",
        "    X, y, target_names = load_and_preprocess_data()\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Extract features using ResNet\n",
        "    resnet = ResNet(input_dim=X_train.shape[1:], output_dim=128, num_blocks=3)\n",
        "    X_train_features = resnet.forward(X_train)\n",
        "    X_test_features = resnet.forward(X_test)\n",
        "\n",
        "    # Train a decision tree on the extracted features\n",
        "    dt = DecisionTree(max_depth=10)\n",
        "    dt.fit(X_train_features, y_train)\n",
        "\n",
        "    # Predict labels for the test set\n",
        "    y_pred = dt.predict(X_test_features)\n",
        "\n",
        "    # Calculate accuracy and display results\n",
        "    accuracy = accuracy_score(y_test, y_pred)  # Compute accuracy\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")  # Print accuracy as a percentage\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))  # Detailed report\n"
      ],
      "metadata": {
        "id": "aIfXfxp6NL1-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main() #intiating for main function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUYmYdcrNPBQ",
        "outputId": "ee4c450a-e465-4717-ac8b-45e70c53659e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 39.15%\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     Ariel Sharon       0.04      0.09      0.06        11\n",
            "     Colin Powell       0.38      0.40      0.39        47\n",
            "  Donald Rumsfeld       0.15      0.09      0.11        22\n",
            "    George W Bush       0.61      0.54      0.57       119\n",
            "Gerhard Schroeder       0.25      0.37      0.30        19\n",
            "      Hugo Chavez       0.12      0.08      0.10        13\n",
            "       Tony Blair       0.23      0.26      0.24        27\n",
            "\n",
            "         accuracy                           0.39       258\n",
            "        macro avg       0.26      0.26      0.25       258\n",
            "     weighted avg       0.41      0.39      0.40       258\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def build_cnn(input_shape, num_classes):\n",
        "    # Build a CNN model\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),  # Convolution layer\n",
        "        MaxPooling2D((2, 2)),  # Max-pooling layer\n",
        "        Flatten(),  # Flatten the output of previous layer\n",
        "        Dense(128, activation='relu'),  # Fully connected layer\n",
        "        Dense(num_classes, activation='softmax')  # Output layer with softmax activation for classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "gNUi3oTfNRAz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and preprocess data\n",
        "X, y, target_names = load_and_preprocess_data()\n",
        "\n",
        "# Add a channel dimension to the data\n",
        "X = X[..., np.newaxis]  # Shape becomes (samples, height, width, 1)\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Build and train the CNN\n",
        "cnn = build_cnn(input_shape=X_train.shape[1:], num_classes=len(target_names))\n",
        "cnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Step 4: Evaluate the CNN\n",
        "cnn_preds = cnn.predict(X_test)\n",
        "cnn_preds_classes = np.argmax(cnn_preds, axis=1)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "cnn_accuracy = accuracy_score(y_test, cnn_preds_classes)\n",
        "print(f\"CNN Accuracy: {cnn_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(y_test, cnn_preds_classes, target_names=target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unns6nKZNaY-",
        "outputId": "3d8c7a3a-981e-4687-cf10-f02dcecf2d78"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.3398 - loss: 1.8443 - val_accuracy: 0.4029 - val_loss: 1.6654\n",
            "Epoch 2/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step - accuracy: 0.3817 - loss: 1.7420 - val_accuracy: 0.4029 - val_loss: 1.6658\n",
            "Epoch 3/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - accuracy: 0.4201 - loss: 1.6716 - val_accuracy: 0.4029 - val_loss: 1.6741\n",
            "Epoch 4/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.3868 - loss: 1.7258 - val_accuracy: 0.4029 - val_loss: 1.6636\n",
            "Epoch 5/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.3675 - loss: 1.7586 - val_accuracy: 0.4029 - val_loss: 1.6735\n",
            "Epoch 6/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.3776 - loss: 1.7513 - val_accuracy: 0.4029 - val_loss: 1.6807\n",
            "Epoch 7/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.4025 - loss: 1.7172 - val_accuracy: 0.4029 - val_loss: 1.6764\n",
            "Epoch 8/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.4256 - loss: 1.6805 - val_accuracy: 0.4029 - val_loss: 1.6979\n",
            "Epoch 9/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.4079 - loss: 1.7185 - val_accuracy: 0.4029 - val_loss: 1.6872\n",
            "Epoch 10/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.4036 - loss: 1.7117 - val_accuracy: 0.4029 - val_loss: 1.6696\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step\n",
            "CNN Accuracy: 46.12%\n",
            "CNN Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     Ariel Sharon       0.00      0.00      0.00        11\n",
            "     Colin Powell       0.00      0.00      0.00        47\n",
            "  Donald Rumsfeld       0.00      0.00      0.00        22\n",
            "    George W Bush       0.46      1.00      0.63       119\n",
            "Gerhard Schroeder       0.00      0.00      0.00        19\n",
            "      Hugo Chavez       0.00      0.00      0.00        13\n",
            "       Tony Blair       0.00      0.00      0.00        27\n",
            "\n",
            "         accuracy                           0.46       258\n",
            "        macro avg       0.07      0.14      0.09       258\n",
            "     weighted avg       0.21      0.46      0.29       258\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kpyIJ918O-GR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3OEkAdMNf53"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}